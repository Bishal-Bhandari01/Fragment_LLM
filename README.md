# Secure LLM from Scratch

A production-ready, security-hardened GPT-style language model implementation optimized for low-end PCs and fast learning.

## ï¿½ Documentation

The documentation is split into specialized guides:

- **ğŸ”° [Quick Start](docs/quickstart.md)** - Get up and running in minutes
- **ğŸ—ï¸ [Architecture](docs/architecture.md)** - System design and component overview
- **ğŸ§  [Model Architecture](docs/model.md)** - Details on the GPT implementation
- **ğŸ“ [Training Guide](docs/training.md)** - How to train models effectively
- **ğŸ”¡ [Tokenizer](docs/tokenizer.md)** - BPE tokenizer details
- **ğŸ’¾ [Dataset Handling](docs/dataset.md)** - Secure data loading
- **ğŸ§¹ [Preprocessing](docs/preprocessing.md)** - Data preparation pipeline
- **ğŸ’¬ [Inference](docs/inference.md)** - Text generation parameters and guide
- **âš™ï¸ [Configuration](docs/configuration.md)** - Full parameter reference
- **ï¿½ğŸ”’ [Security](docs/security.md)** - Detailed security features and rules
- **ğŸ“– [API Reference](docs/api-reference.md)** - Class and function documentation

## ğŸ”’ Security Features

- **OWASP ASVS 4.0 Compliant**: Input validation, secure deserialization, path traversal prevention
- **CWE Top 25 Mitigations**: Protection against critical vulnerabilities
- **CIS Benchmark Aligned**: Follows security best practices
- **No Pickle Usage**: JSON-based serialization to prevent arbitrary code execution
- **Resource Limits**: Protection against DoS and resource exhaustion
- **Atomic Operations**: Safe checkpoint saving and loading

## ğŸš€ Optimizations for Low-End PCs

- **Reduced Model Size**: 6 layers, 384 embedding dimension (38M parameters)
- **Gradient Accumulation**: Effective batch size 64 with actual batch 16
- **Mixed Precision Training**: FP16 to reduce memory usage by 50%
- **Efficient Architecture**: Combined QKV projections, optimized attention
- **Memory-Efficient Data Loading**: Streaming without loading full dataset

## ğŸ¯ Quick Start Summary

For detailed instructions, see the [Quick Start Guide](docs/quickstart.md).

```bash
# 1. Install
pip install torch pandas tqdm

# 2. Prepare Data
python scripts/preprocessor.py

# 3. Train
python train.py --epochs 10

# 4. Generate
python src/inference.py --interactive
```

## ğŸ“‚ Project Structure

```
Fragment_LLM/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ tokenizer.py        # Tokenizer
â”‚   â”œâ”€â”€ dataset.py          # Data handling
â”‚   â”œâ”€â”€ model.py            # Neural network
â”‚   â”œâ”€â”€ trainer.py          # Training loop
â”‚   â””â”€â”€ inference.py        # Generation
â”œâ”€â”€ scripts/                # Helper scripts
â”‚   â””â”€â”€ preprocessor.py     # Data prep
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ quickstart.md       # Getting started
â”‚   â”œâ”€â”€ architecture.md     # System design
â”‚   â””â”€â”€ ...                 # Feature docs
â”œâ”€â”€ train.py                # Main entry point
â”œâ”€â”€ SECURITY_RULES.md       # Security guidelines
â””â”€â”€ README.md               # This file
```

## ğŸ¤ Contributing

See [Security documentation](docs/security.md) before contributing. Ensure all security rules are followed:
1. Input validation for all public methods
2. No unsafe deserialization (pickle)
3. Path checks for file operations
4. Resource limits enforcement

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸŒŸ Acknowledgments

- Based on GPT architecture from "Attention is All You Need"
- Optimized for educational and research purposes
- Security hardened following OWASP, CWE, and CIS guidelines